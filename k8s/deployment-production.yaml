---
# Namespace
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    name: production

---
# ConfigMap for inference service configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: inference-config
  namespace: production
data:
  # MLflow configuration
  MLFLOW_TRACKING_URI: "http://mlflow-tracking:5000"
  MODEL_STAGE: "Production"
  
  # Service configuration
  LOG_LEVEL: "INFO"
  PORT: "8000"
  WORKERS: "4"
  
  # Model inference settings
  INFERENCE_TIMEOUT: "30"
  MAX_BATCH_SIZE: "32"

---
# Secret for sensitive data
apiVersion: v1
kind: Secret
metadata:
  name: inference-secrets
  namespace: production
type: Opaque
stringData:
  # Configure these values:
  # MLFLOW_REGISTRY_URI: "your-registry-uri"
  # AZURE_STORAGE_CONNECTION_STRING: "your-connection-string"
  MLFLOW_REGISTRY_URI: "http://mlflow-tracking:5000"

---
# Service Account
apiVersion: v1
kind: ServiceAccount
metadata:
  name: inference-service
  namespace: production

---
# Role for inference service
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: inference-service-role
  namespace: production
rules:
  # Allow reading ConfigMaps and Secrets
  - apiGroups: [""]
    resources: ["configmaps", "secrets"]
    verbs: ["get", "list", "watch"]
  # Allow reading service information
  - apiGroups: [""]
    resources: ["services"]
    verbs: ["get", "list"]

---
# RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: inference-service-binding
  namespace: production
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: inference-service-role
subjects:
  - kind: ServiceAccount
    name: inference-service
    namespace: production

---
# Deployment with enhanced health checks and resource management
apiVersion: apps/v1
kind: Deployment
metadata:
  name: inference-service
  namespace: production
  labels:
    app: inference-service
    version: v1
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0  # Zero downtime deployment
  selector:
    matchLabels:
      app: inference-service
  template:
    metadata:
      labels:
        app: inference-service
        version: v1
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: inference-service
      
      # Pod anti-affinity for high availability
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - inference-service
                topologyKey: kubernetes.io/hostname
      
      # Graceful shutdown
      terminationGracePeriodSeconds: 30
      
      # Init container for pre-flight checks
      initContainers:
        - name: check-mlflow
          image: curlimages/curl:latest
          command: ['sh', '-c']
          args:
            - |
              until curl -sf http://mlflow-tracking:5000/health || [ $SECONDS -ge 30 ]; do
                echo "Waiting for MLflow to be ready..."
                sleep 2
              done
              echo "MLflow is ready"

      containers:
        - name: inference-service
          # Use your ACR image here - will be updated by CI/CD
          image: inference-service:latest
          imagePullPolicy: Always
          
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP
          
          # Environment variables from ConfigMap and Secret
          envFrom:
            - configMapRef:
                name: inference-config
            - secretRef:
                name: inference-secrets
          
          # Additional environment variables
          env:
            - name: MODEL_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.labels['model-name']
              optional: true
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          
          # Liveness probe: Restart container if service is unhealthy
          livenessProbe:
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 3
          
          # Readiness probe: Only route traffic to healthy pods
          readinessProbe:
            httpGet:
              path: /ready
              port: http
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 2
          
          # Startup probe: Allow time for model loading
          startupProbe:
            httpGet:
              path: /health
              port: http
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 5
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 30  # 30 * 5 = 150 seconds max startup
          
          # Resource requests and limits
          resources:
            requests:
              cpu: "500m"
              memory: "512Mi"
            limits:
              cpu: "2000m"
              memory: "2Gi"
          
          # Security context
          securityContext:
            runAsNonRoot: true
            runAsUser: 1000
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            capabilities:
              drop:
                - ALL
          
          # Volume mounts
          volumeMounts:
            - name: tmp
              mountPath: /tmp
            - name: cache
              mountPath: /app/cache

      # Volumes
      volumes:
        - name: tmp
          emptyDir: {}
        - name: cache
          emptyDir: {}
      
      # Image pull secrets (uncomment if using private registry)
      # imagePullSecrets:
      #   - name: acr-secret

---
# Service with load balancer
apiVersion: v1
kind: Service
metadata:
  name: inference-service
  namespace: production
  labels:
    app: inference-service
spec:
  type: LoadBalancer
  selector:
    app: inference-service
  ports:
    - name: http
      port: 80
      targetPort: http
      protocol: TCP
    - name: https
      port: 443
      targetPort: http
      protocol: TCP
  
  # Session affinity for stateful clients (optional)
  sessionAffinity: None
  
  # Load balancer configuration
  loadBalancerSourceRanges: []  # Allow all by default, restrict as needed

---
# HorizontalPodAutoscaler
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: inference-service-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: inference-service
  minReplicas: 3
  maxReplicas: 10
  metrics:
    # Scale on CPU usage
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    # Scale on memory usage
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 50
          periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
        - type: Percent
          value: 100
          periodSeconds: 15
        - type: Pods
          value: 2
          periodSeconds: 15
      selectPolicy: Max

---
# PodDisruptionBudget for high availability
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: inference-service-pdb
  namespace: production
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: inference-service

---
# NetworkPolicy for security (optional)
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: inference-service-policy
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: inference-service
  policyTypes:
    - Ingress
    - Egress
  ingress:
    # Allow from ingress controller
    - from:
        - namespaceSelector:
            matchLabels:
              name: ingress-nginx
      ports:
        - protocol: TCP
          port: 8000
    # Allow from same namespace
    - from:
        - podSelector: {}
      ports:
        - protocol: TCP
          port: 8000
  egress:
    # Allow to MLflow
    - to:
        - podSelector:
            matchLabels:
              app: mlflow-tracking
      ports:
        - protocol: TCP
          port: 5000
    # Allow DNS
    - to:
        - namespaceSelector: {}
      ports:
        - protocol: UDP
          port: 53
    # Allow to external registries and APIs
    - to:
        - namespaceSelector: {}
      ports:
        - protocol: TCP
          port: 443

---
# Monitoring: ServiceMonitor (if using Prometheus)
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: inference-service
  namespace: production
  labels:
    release: prometheus
spec:
  selector:
    matchLabels:
      app: inference-service
  endpoints:
    - port: http
      path: /metrics
      interval: 30s
      timeout: 10s
